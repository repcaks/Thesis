{"paragraphs":[{"text":"%dep\r\nz.load(\"/usr/hdp/3.0.1.0-187/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.0.1.0-187.jar\")\r\nz.load(\"/jars/spark-sql-kafka-0-10_2.11-2.3.2.3.1.0.6-1.jar\")\r\nz.load(\"/jars/kafka-clients-1.1.0.jar\")\r\nz.load(\"/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar\")","user":"anonymous","dateUpdated":"2021-01-03T21:52:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@4a8759fe\n"}]},"apps":[],"jobName":"paragraph_1605026525982_-1563716803","id":"20201110-164205_542103893","dateCreated":"2020-11-10T16:42:05+0000","dateStarted":"2021-01-03T21:52:11+0000","dateFinished":"2021-01-03T21:52:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20672"},{"text":"%spark2\r\n\r\nimport kafka.serializer.StringDecoder\r\nimport org.apache.kafka.common.serialization.StringDeserializer\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.types.{DateType, DoubleType, IntegerType, StructField, StructType, StringType}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.streaming.kafka.KafkaUtils\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\nimport com.hortonworks.hwc.HiveWarehouseSession\r\nimport com.hortonworks.hwc.HiveWarehouseSession._\r\n\r\nval topics = \"twitter\"\r\nval brokers = \"sandbox-hdp.hortonworks.com:6667\"\r\n\r\nval spark = SparkSession \r\n        .builder\r\n        .appName(\"Twitter Streaming App\")\r\n        .master(\"yarn\")\r\n        .enableHiveSupport()\r\n        .config(\"spark.sql.hive.llap\", \"true\")\r\n        .config(\"spark.datasource.hive.warehouse.exec.results.max\",\"10000\")\r\n        .config(\"spark.sql.hive.hiveserver2.jdbc.url\",\"jdbc:hive2://sandbox-hdp.hortonworks.com:2181/default;password=hive;serviceDiscoveryMode=zooKeeper;user=hive;zooKeeperNamespace=hiveserver2\")\r\n        .getOrCreate()\r\n\r\n\r\nval hive = HiveWarehouseSession.session(spark).build()\r\n\r\nhive.setDatabase(\"stockexchange\")\r\n\r\nhive.createTable(\"twitter\").ifNotExists().column(\"`date`\", \"String\").column(\"id\", \"string\").column(\"source\", \"string\").column(\"text\", \"String\").column(\"user_account_created_time\", \"string\").column(\"user_favourites_count\", \"int\").column(\"user_followers_count\", \"int\").column(\"user_friends_count\", \"int\").column(\"user_id\", \"string\").column(\"user_name\", \"string\").column(\"user_statuses_count\", \"string\").create()\r\n\r\n\r\nval kafkaParams = Map[String,String](\r\n      \"metadata.broker.list\"->brokers,\r\n      \"key.deserializer\" -> classOf[StringDeserializer].toString(),\r\n      \"value.deserializer\"-> classOf[StringDeserializer].toString(),\r\n      \"auto.offset.reset\" -> \"largest\"\r\n    )\r\n\r\nval batchInterval= Seconds(10)\r\n\r\n\r\nval ssc = new StreamingContext(spark.sparkContext, batchInterval)\r\nssc.checkpoint(\"/warehouse/checkpoint/twitter/\")\r\n \r\nprintln(\"Stream Creating\")\r\n\r\nval messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\r\n      ssc, kafkaParams, Set(topics))\r\n      \r\nimport spark.implicits._\r\n\r\nval lines = messages.map(_._2)\r\n\r\nlines.foreachRDD(\r\n      rdd => {\r\n\r\n        val rawDF = rdd.toDF(\"msg\")\r\n\r\n        if(!rawDF.rdd.isEmpty()) {\r\n\r\n          println(\"Raw\")\r\n          \r\n          val schema = spark.read.json(rawDF.select(\"msg\").as[String]).schema\r\n            \r\n          val df = rawDF.select(from_json($\"msg\",schema).as(\"s\")).select(\"s.*\")\r\n          \r\n          val df2 = df.withColumn(\"user_favourites_count\",col(\"user_favourites_count\").cast(IntegerType))\r\n                      .withColumn(\"user_followers_count\",col(\"user_followers_count\").cast(IntegerType))\r\n                      .withColumn(\"user_friends_count\",col(\"user_friends_count\").cast(IntegerType))\r\n                      .withColumn(\"user_statuses_count\",col(\"user_statuses_count\").cast(IntegerType))\r\n\r\n          \r\n          println(\"test\")\r\n          df2.printSchema()\r\n          df2.show()\r\n\r\n          \r\n          println(\"Saving\")\r\n          df2.write.format(HIVE_WAREHOUSE_CONNECTOR).mode(\"append\").option(\"table\", \"twitter\").save()\r\n          println(\"Saved\")\r\n        }\r\n      }\r\n    )\r\n\r\nssc.start()\r\nssc.awaitTermination()","user":"anonymous","dateUpdated":"2021-01-03T21:52:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:274)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:258)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:233)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:229)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:135)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:437)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1609209551319_1743359686","id":"20201229-023911_869608592","dateCreated":"2020-12-29T02:39:11+0000","dateStarted":"2021-01-03T21:52:23+0000","dateFinished":"2021-01-03T21:53:05+0000","status":"ABORT","errorMessage":"org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:274)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:258)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:233)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:229)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:135)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:437)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n","progressUpdateIntervalMs":500,"$$hashKey":"object:20673"},{"text":"%spark2\r\n\r\nimport kafka.serializer.StringDecoder\r\nimport org.apache.kafka.common.serialization.StringDeserializer\r\nimport org.apache.spark.sql.SparkSession\r\nimport org.apache.spark.sql.types.{DateType, DoubleType, IntegerType, StructField, StructType, StringType}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.streaming.kafka.KafkaUtils\r\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\r\n\r\n\r\n\r\nval topics = \"twitter\"\r\nval brokers = \"sandbox-hdp.hortonworks.com:6667\"\r\n\r\nval spark = SparkSession \r\n        .builder\r\n        .appName(\"Spark Stock Price Prediction\")\r\n        .master(\"yarn\")\r\n        .enableHiveSupport()\r\n        .config(\"spark.sql.hive.llap\", \"true\")\r\n        .config(\"spark.yarn.am.cores\", \"1\")\r\n        .config(\"spark.datasource.hive.warehouse.exec.results.max\",\"10000\")\r\n        .config(\"spark.sql.hive.hiveserver2.jdbc.url\",\"jdbc:hive2://sandbox-hdp.hortonworks.com:2181/default;password=hive;serviceDiscoveryMode=zooKeeper;user=hive;zooKeeperNamespace=hiveserver2\")\r\n        .getOrCreate()\r\n\r\n\r\nval hive = com.hortonworks.spark.sql.hive.llap.HiveWarehouseBuilder.session(spark).build()\r\n\r\nval kafkaParams = Map[String,String](\r\n      \"metadata.broker.list\"->brokers,\r\n      \"key.deserializer\" -> classOf[StringDeserializer].toString(),\r\n      \"value.deserializer\"-> classOf[StringDeserializer].toString(),\r\n      \"auto.offset.reset\" -> \"largest\"\r\n    )\r\n\r\nval batchInterval= Seconds(10)\r\n\r\nval schema = new StructType()\r\n      .add(StructField(\"user_id\", StringType, true))\r\n      .add(StructField(\"user_name\", StringType, true))\r\n      .add(StructField(\"user_followers_count\", IntegerType, true))\r\n      .add(StructField(\"user_statuses_count\", IntegerType, true))\r\n      .add(StructField(\"favourites_count\", IntegerType, true))\r\n      .add(StructField(\"time\", DateType, true))\r\n      .add(StructField(\"id\", IntegerType, true))\r\n      .add(StructField(\"source\", StringType, true))\r\n      .add(StructField(\"text\", StringType, true))\r\n      .add(StructField(\"user_friends_count\", IntegerType, true))\r\n\r\n\r\nval ssc = new StreamingContext(spark.sparkContext, batchInterval)\r\n \r\nprintln(\"Stream Creating\")\r\n\r\nval messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\r\n      ssc, kafkaParams, Set(topics))\r\n      \r\nimport spark.implicits._\r\n\r\nval lines = messages.map(_._2)\r\n\r\nlines.foreachRDD(\r\n      rdd => {\r\n\r\n        val rawDF = rdd.toDF(\"msg\")\r\n\r\n        if(!rawDF.rdd.isEmpty()) {\r\n\r\n          println(\"Raw\")\r\n          rawDF.printSchema()\r\n          \r\n          val schema = spark.read.json(rawDF.select(\"msg\").as[String]).schema\r\n            \r\n          val df = rawDF.select(from_json($\"msg\",schema).as(\"s\")).select(\"s.*\")\r\n          \r\n          df.show()\r\n          \r\n          println(\"Saving\")\r\n        //   df.write.format(\"com.hortonworks.spark.sql.hive.llap.HiveWarehouseConnector\").mode(\"append\").option(\"table\", \"stockexchange.twitter\").save\r\n          println(\"Saved\")\r\n        }\r\n      }\r\n    )\r\n\r\nssc.start()\r\nssc.awaitTermination()","user":"anonymous","dateUpdated":"2020-12-30T17:10:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1609348241132_-790498542","id":"20201230-171041_1994286810","dateCreated":"2020-12-30T17:10:41+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20674"},{"text":"\n%spark2.pyspark\n\nfrom pyspark.sql import SparkSession\nsc.addPyFile(\"/usr/hdp/3.0.1.0-187/hive_warehouse_connector/pyspark_hwc-1.0.0.3.0.1.0-187.zip\")\nfrom pyspark_llap import HiveWarehouseSession\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\nimport pandas as pd\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport json\nimport os\n\nos.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3.6\"\nos.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3.6\"\n\nKAFKA_TOPIC_NAME_CONS = \"twitter\"\nKAFKA_BOOTSTRAP_SERVERS_CONS = 'sandbox-hdp.hortonworks.com:6667'\n\nKAFKA_PARAMS = {\n    \"bootstrap.servers\": \"sandbox-hdp.hortonworks.com:6667\",\n    \"group.id\": \"Deserialize\"\n}\n\nprint(\"Real-Time Data Pipeline Started ...\")\n\nspark = SparkSession \\\n        .builder \\\n        .appName(\"Spark Stock Price Prediction\") \\\n        .master(\"yarn\") \\\n        .enableHiveSupport() \\\n        .config(\"spark.sql.hive.llap\", \"true\") \\\n        .config(\"spark.datasource.hive.warehouse.exec.results.max\",\"10000\") \\\n        .config(\"spark.sql.hive.hiveserver2.jdbc.url\",\"jdbc:hive2://sandbox-hdp.hortonworks.com:2181/default;password=hive;serviceDiscoveryMode=zooKeeper;user=hive;zooKeeperNamespace=hiveserver2\") \\\n                .config(\"spark.jars\",\n                \"/jars/kafka-clients-1.1.0.jar/spark-sql-kafka-0-10_2.11-2.3.2.3.1.0.6-1.jar,/jars/kafka-clients-1.1.0.jar/SparkWarsawRealTimePipeline//kafka-clients-1.1.0.jar,/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar\") \\\n        .config(\"spark.executor.extraClassPath\",\n                \"/jars/kafka-clients-1.1.0.jar/spark-sql-kafka-0-10_2.11-2.3.2.3.1.0.6-1.jar:/jars/kafka-clients-1.1.0.jar/SparkWarsawRealTimePipeline//kafka-clients-1.1.0.jar:/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar\") \\\n        .config(\"spark.executor.extraLibrary\",\n                \"/jars/kafka-clients-1.1.0.jar/spark-sql-kafka-0-10_2.11-2.3.2.3.1.0.6-1.jar:/jars/kafka-clients-1.1.0.jar/SparkWarsawRealTimePipeline//kafka-clients-1.1.0.jar:/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar\") \\\n        .config(\"spark.driver.extraClassPath\",\n               \"/jars/kafka-clients-1.1.0.jar/spark-sql-kafka-0-10_2.11-2.3.2.3.1.0.6-1.jar:/jars/kafka-clients-1.1.0.jar/SparkWarsawRealTimePipeline//kafka-clients-1.1.0.jar:/jars/spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar\") \\\n        .getOrCreate()\n        \nhive = HiveWarehouseSession.session(spark).build()\n\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nssc = StreamingContext(sc, 5)\n\ndef decoder(s):\n    if s is None:\n        return None\n\n    loaded_json = json.loads(s.decode('utf-8'))\n    loaded_json[\"ID\"] = int.from_bytes(base64.b64decode(loaded_json['ID']), \"big\")\n    return loaded_json\n\n\nkafka_stream = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC_NAME_CONS], KAFKA_PARAMS, valueDecoder=decoder)\n\n\nkafka_stream.foreachRDD(lambda rdd: rdd.collect())\n\nssc.start()\nssc.awaitTermination()\n\n# transaction_detail_df = spark \\\n#     .readStream \\\n#     .format(\"kafka\") \\\n#     .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS_CONS) \\\n#     .option(\"subscribe\", KAFKA_TOPIC_NAME_CONS) \\\n#     .option(\"startingOffsets\", \"latest\") \\\n#     .load()\n    \n# print(\"Printing Schema of transaction_detail_df: \")\n# transaction_detail_df.printSchema()\n\n\n# print(\"Write into HDFS: \")\n# write_stream = transaction_detail_df.writeStream \\\n#       .trigger(processingTime='5 seconds') \\\n#       .format(\"json\") \\\n#       .option(\"path\", \"/data/json/trans_detail_raw_data\") \\\n#       .option(\"checkpointLocation\", \"/data/checkpoint/trans_detail_raw_data\") \\\n#       .start()\n\n# write_stream.awaitTermination()\n\n# print(\"Real-Time Data Pipeline Completed.\")","user":"anonymous","dateUpdated":"2020-12-30T15:21:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Real-Time Data Pipeline Started ...\n"},{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o129.awaitTermination.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/streaming/kafka.py\", line 403, in <lambda>\n    func = lambda r, rdd: old_func(rdd)\n  File \"<stdin>\", line 61, in <lambda>\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 834, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, sandbox-hdp.hortonworks.com, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/streaming/kafka.py\", line 131, in funcWithoutMessageHandler\n  File \"<stdin>\", line 54, in decoder\nAttributeError: 'function' object has no attribute 'b64decode'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/streaming/kafka.py\", line 131, in funcWithoutMessageHandler\n  File \"<stdin>\", line 54, in decoder\nAttributeError: 'function' object has no attribute 'b64decode'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o129.awaitTermination.\\n', JavaObject id=o260), <traceback object at 0x7f5f32362108>)"}]},"apps":[],"jobName":"paragraph_1606861586367_751048895","id":"20201201-222626_575595635","dateCreated":"2020-12-01T22:26:26+0000","dateStarted":"2020-12-30T15:21:37+0000","dateFinished":"2020-12-30T15:22:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:20675"},{"text":"%spark2\n","user":"anonymous","dateUpdated":"2020-12-30T14:42:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1609339373067_723769561","id":"20201230-144253_1067518522","dateCreated":"2020-12-30T14:42:53+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:20676"}],"name":"Twitter Pipeline","id":"2FPJGY8YF","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}